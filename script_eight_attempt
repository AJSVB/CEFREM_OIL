from torch.utils.data import Dataset, DataLoader, sampler
from pathlib import Path
from PIL import Image
import matplotlib.pyplot as plt
hyper = 3
class CloudDataset(Dataset):
    def __init__(self, _dir, gt_dir, pytorch=True):
        super().__init__()
        
        # Loop through the files in red folder and combine, into a dictionary, the other bands
        self.files = [self.combine_files(f, gt_dir) for f in _dir.iterdir() if not f.is_dir()]
        self.pytorch = pytorch
        #self.correction = 0
    def combine_files(self, r_file: Path,  gt_dir):
        
        files = {'red': r_file, 
                 'gt': gt_dir/r_file.name.replace('red', 'gt')}

        return files
                                       
    def __len__(self):
        
        return len(self.files) 
    
     
    def open_as_array(self, idx, invert=False, include_nir=False):
        
      #  raw_rgb = np.stack([np.array(Image.open(self.files[idx]['red'])),
      #                     ], axis=2)
        f = open(self.files[idx]['red'], 'rb')
        temp = np.load(f)
        #print(np.max(temp))
        #print(np.min(temp))
        return temp
    
    def open_mask(self, idx, add_dims=False):
        f = open(self.files[idx]['gt'], 'rb')
        temp = np.load(f)
     #   if (np.max(temp) != 0) :
     #       print(np.max(temp))
#	if temp.shape != (256,256):
#            temp = (np.zeros((256,256))[] = temp)
        return temp
    
    def __getitem__(self, idx):
        temp = self.open_as_array(idx, invert=self.pytorch, include_nir=True)
        x = torch.tensor(temp, dtype=torch.float32)

        temp = self.open_mask(idx, add_dims=False)
        y = torch.tensor(temp, dtype=torch.torch.int64)
        return x, y
    
    def open_as_pil(self, idx):
        
        arr = 256*self.open_as_array(idx)
        
        return Image.fromarray(arr.astype(np.uint8))
    
    def __repr__(self):
        s = 'Dataset class with {} files'.format(self.__len__())

        return s


base_path = Path("./Documents")
#data = CloudDataset(base_path/'secondattemptX', base_path/'secondattemptY')
data = CloudDataset(base_path/'6X', base_path/'6Y')
import torch 
import numpy as np
import cv2
len(data)
ratio = .75
train_ds, valid_ds = torch.utils.data.random_split(data, (int(len(data)*ratio), len(data) - (int(len(data)*ratio))))


train_dl = DataLoader(data, batch_size=64, shuffle=True)
valid_dl = [] #DataLoader(valid_ds, batch_size=4, shuffle=True)

import random

import time
def train(model, train_dl, valid_dl, loss_fn, optimizer, acc_fn, epochs=1,go='train'):
    start = time.time()
    model#.cuda()

    train_loss, valid_loss = [], []

    best_acc = 0.0
    def model1(x):
        x = x[:,torch.randperm(10)]
        a = random.random()
        if a < .25:
            f = lambda b : torch.flip(b,[2,3])
        elif a < .5:
            f = lambda b : torch.flip(b,[3])
        elif a < .75:
            f = lambda b : torch.flip(b,[2])
        else:
            f = lambda b : b
        fx = model(f(x))#['out']
      #  print(fx)
        return fx 
        return f(fx)# mirror
#        return torch.sigmoid(model(x))
    for epoch in range(epochs):
        print('Epoch {}/{}'.format(epoch, epochs - 1))
        print('-' * 10)

        for phase in [go]:  
            if phase == 'train':
                model.train(True)  # Set trainind mode = true
                dataloader = train_dl
            else:
                model.train(False)  # Set model to evaluate mode
                dataloader = train_dl#valid_dl

            running_loss = 0.0
            running_acc = 0.0
            conf_mat = np.zeros((hyper,hyper))
            step = 0
            #timer=time.time()
            # iterate over data
            for x, y in dataloader:
               # x = x.cuda()
               # y = y.cuda()
                step += 1
                z = torch.empty(y.size()[0])
                for e,a in enumerate(y):
                 #   print(np.max(a))
                    t1 = (a.numpy()==1.).any()
                    t2 = (a.numpy()==2.).any()
                    t0 = t2==False and t1 == False
                   # print(t1)
                   # print(t2)
                    z[e] = t1 + t2*2 - (t1 and t2)*2
                    #print(z[e])
                  #  z[e] = torch.LongTensor([t0*1.,t1*1.,t2*1.])
               # print(z)
                temp = y
                y=z.long()
                # forward pass
                if phase == 'train':
                    loss = 1
                    if 1==1:
                    #while(loss>.1):
           #
                    # zero the gradients
                        optimizer.zero_grad()
                    
                        outputs = model1(x)
                    
                        loss = loss_fn(outputs, y)

                    # the backward pass frees the graph memory, so there is no 
                    # need for torch.no_grad in this training pass
                        loss.backward()
                        optimizer.step()
                    # scheduler.step()
                  #      print(loss)
                    
                else:
                    with torch.no_grad():
                        outputs = model1(x)
                        loss = loss_fn(outputs, y.long())
                        
                        if loss==.1:
                            for i in range(len(x)):
                                if(np.sum(np.array(y[i])>0)>=0):
                                    #print(x.shape)
                                    fig, axs = plt.subplots(3,4)
                                    axs[0,0].imshow(x[i,0,:,:])
                                    axs[0,1].imshow(x[i,1,:,:])
                                    axs[0,2].imshow(x[i,2,:,:])
                                    axs[1,0].imshow(x[i,3,:,:])
                                    axs[1,1].imshow(x[i][4])
                                    axs[1,2].imshow(x[i][5])
                                    axs[2,0].imshow(x[i][6])
                                    axs[2,1].imshow(x[i][7])
                                    axs[2,2].imshow(x[i][8])
                                    axs[0,3].imshow(x[i][9])
                                  #  axs[1,3].imshow(x[i][10])
                                  #  axs[2,3].imshow(x[i][11])               
                                  #  axs[3,0].imshow(x[i][12])
                                   # axs[3,1].imshow(F.sigmoid(outputs[i][1]-outputs[i][0]))
                                   # axs[1,3].imshow(outputs[i])
                                    #print(outputs[i])
                                    print(y[i])
                                    axs[2,3].imshow(temp[i])
                                   # plt.savefig(str(loss)+str(i)+".jpg")
                                    plt.show()
                        
                # stats - whatever is the phase
                acc = acc_fn(outputs, y)
                from sklearn.metrics import confusion_matrix
                temp =confusion_matrix(y,outputs.argmax(dim=1))
                conf_mat[:temp.shape[0],:temp.shape[1]] += temp 
                running_acc  += acc*len(y)
                running_loss += loss*acc*len(y) 

            #    if step % 10 == 0:
                    # clear_output(wait=True)
           #         print('Current step: {}  Loss: {}  Acc: {}  AllocMem (Mb): {}'.format(step, loss, acc, torch.cuda.memory_allocated()/1024/1024))
                    # print(torch.cuda.memory_summary())
                
            epoch_loss = running_loss / len(dataloader.dataset)
            epoch_acc = running_acc / len(dataloader.dataset)
            print(conf_mat)
            print('{} Loss: {:.4f} Acc: {}'.format(phase, epoch_loss, epoch_acc))

            train_loss.append(epoch_loss) if phase=='train' else valid_loss.append(epoch_loss)

    time_elapsed = time.time() - start
    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))    
    
    return train_loss, valid_loss, model, optimizer    

def acc_metric(predb, yb):
   # return (predb.argmax(dim=1) == yb.cuda()).float().mean()

    return (predb.argmax(dim=1) == yb).float().mean()


#https://github.com/usuyama/pytorch-unet
import torch
import torch.nn as nn
from torchvision import models

def convrelu(in_channels, out_channels, kernel, padding):
    return nn.Sequential(
        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),
        nn.ReLU(inplace=True),
    )


class ResNetUNet(nn.Module):
    def __init__(self, n_class):
        super().__init__()

        self.base_model = models.resnet18(pretrained=False)
        self.base_layers = list(self.base_model.children())
        print(*self.base_layers[:3])
        self.layer0 = nn.Sequential(
nn.Conv2d(13, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),
            nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True), 
            nn.ReLU(inplace=True)) # size=(N, 64, x.H/2, x.W/2)
        self.layer0_1x1 = convrelu(64, 64, 1, 0)
        self.layer1 = nn.Sequential(*self.base_layers[3:5]) # size=(N, 64, x.H/4, x.W/4)
        self.layer1_1x1 = convrelu(64, 64, 1, 0)
        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)
        self.layer2_1x1 = convrelu(128, 128, 1, 0)
        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)
        self.layer3_1x1 = convrelu(256, 256, 1, 0)
        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)
        self.layer4_1x1 = convrelu(512, 512, 1, 0)

        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)

        self.conv_up3 = convrelu(256 + 512, 512, 3, 1)
        self.conv_up2 = convrelu(128 + 512, 256, 3, 1)
        self.conv_up1 = convrelu(64 + 256, 256, 3, 1)
        self.conv_up0 = convrelu(64 + 256, 128, 3, 1)

        self.conv_original_size0 = convrelu(13, 64, 3, 1)
        self.conv_original_size1 = convrelu(64, 64, 3, 1)
        self.conv_original_size2 = convrelu(64 + 128, 64, 3, 1)

        self.conv_last = nn.Conv2d(64, n_class, 1)

    def forward(self, input):
        x_original = self.conv_original_size0(input)
        x_original = self.conv_original_size1(x_original)

        layer0 = self.layer0(input)
        layer1 = self.layer1(layer0)
        layer2 = self.layer2(layer1)
        layer3 = self.layer3(layer2)
        layer4 = self.layer4(layer3)

        layer4 = self.layer4_1x1(layer4)
        x = self.upsample(layer4)
        layer3 = self.layer3_1x1(layer3)
        x = torch.cat([x, layer3], dim=1)
        x = self.conv_up3(x)

        x = self.upsample(x)
        layer2 = self.layer2_1x1(layer2)
        x = torch.cat([x, layer2], dim=1)
        x = self.conv_up2(x)

        x = self.upsample(x)
        layer1 = self.layer1_1x1(layer1)
        x = torch.cat([x, layer1], dim=1)
        x = self.conv_up1(x)

        x = self.upsample(x)
        layer0 = self.layer0_1x1(layer0)
        x = torch.cat([x, layer0], dim=1)
        x = self.conv_up0(x)

        x = self.upsample(x)
        x = torch.cat([x, x_original], dim=1)
        x = self.conv_original_size2(x)

        out = self.conv_last(x)
       # print(out)
       # print(torch.sigmoid(out))
        return out# torch.sigmoid(out)
torch.set_num_threads(8)
from adabelief_pytorch import AdaBelief



import numpy
import torch
import torch.nn as nn
import torch.nn.functional as F




#PyTorch
ALPHA = 0.9
BETA = 0.1
GAMMA = 1
class IoULoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(IoULoss, self).__init__()

    def forward(self, inputs, targets, smooth=1):
        
        #comment out if your model contains a sigmoid or equivalent activation layer

        
        #flatten label and prediction tensors
       # print(inputs.shape)
       # print(inputs[:,0:1].shape)
        inputs = (inputs[:,1:2].reshape(-1) - inputs[:,0:1].reshape(-1)) 
        inputs = (inputs>3)*1.      
        targets = targets.view(-1)
        
        #intersection is equivalent to True Positive count
        #union is the mutually inclusive area of all labels & predictions 
        intersection = (inputs * targets).sum()
        total = (inputs + targets).sum()
        union = total - intersection 
        
        IoU = (intersection + smooth)/(union + smooth)
                
        return 1 - IoU
class FocalTverskyLoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(FocalTverskyLoss, self).__init__()

    def forward(self, inputs, targets, smooth=1, alpha=ALPHA, beta=BETA, gamma=GAMMA):
        
        #comment out if your model contains a sigmoid or equivalent activation layer
      #  inputs = F.sigmoid(inputs)       
        
        #flatten label and prediction tensors
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        
        #True Positives, False Positives & False Negatives
        TP = (inputs * targets).sum()    
        FP = ((1-targets) * inputs).sum()
        FN = (targets * (1-inputs)).sum()
        print()
        print(TP)
        print(FP)
        print(FN)
        Tversky = (TP ) / (TP + alpha*FP + beta*FN + smooth)  
        FocalTversky = (1 - Tversky)**gamma
        print(FocalTversky)                       
        return FocalTversky

class DiceLoss(nn.Module):
    def __init__(self, weight=None, size_average=True):
        super(DiceLoss, self).__init__()

    def forward(self, inputs, targets, smooth=1):
        
        #comment out if your model contains a sigmoid or equivalent activation layer
      #  inputs = F.sigmoid(inputs)       
        
        #flatten label and prediction tensors
        inputs = inputs.view(-1)
        targets = targets.view(-1)
        
        intersection = (inputs * targets).sum()   
        print(intersection)
        print(inputs.sum())         
        print(targets.sum())

        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  
        print(1 - dice)
        return 1 - dice


lr = 1e-4
#92, 238, 334,11
#295, 86, 224
weight = torch.tensor([1., 295./86,295./224])#, 334/11])
loss_fn = nn.CrossEntropyLoss(weight=weight)
#loss_fn = IoULoss()
#loss_fn = FocalTverskyLoss()
#loss_fn = DiceLoss()
#loss_fn = lambda x,y : F.binary_cross_entropy(x*1.,y*1.)
#loss_fn = nn.NLLLoss(weight=weight)
#loss_fn = nn.BCEWithLogitsLoss(weight=weight)
#loss_fn = nn.MSELoss()#(weight=weight)
class supa(nn.Module):
    def __init__(self, model):
        super().__init__()
        self.c = 3
        self.model = model
        self.d =2
        self.drp1 = random.random()/16
        self.drp2 = random.random()/16
        self.preprocessing = .7 # random.random()
        print(self.drp1)
        print(self.drp2)
        print(self.preprocessing)
        if self.preprocessing>2./3:
            self.d = 10 # random.random()/2
            print(self.d)
        self.otherpre = nn.Conv2d(10, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))#, bias=False)
        self.pre = nn.Conv2d(1, self.c, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))#, bias=False)
        self.pre2 =         nn.ReLU()
        self.pre3 = nn.Dropout(self.drp1)
        self.post = nn.Linear(1000,hyper)
        self.post1 = nn.Dropout(self.drp2)
       # self.preLaurent0 = nn.MaxPool3d((10,1,1))
       # self.preLaurent1 = nn.AvgPool3d((10,1,1))
        #self.preLaurent0 = torch.var()#nn.LPPool2d(1/self.d,(10,1))
        self.preLaurent1 = nn.LPPool2d(self.d,(10,1))
        self.preLaurent2 = nn.LPPool2d(1,(10,1))
        self.norm = nn.BatchNorm2d(10)
    def forward(self, input):
        a = input.shape[0]
        b = input.shape[1]
        if self.preprocessing <1./3:
    
            x = self.pre(input.view(a*b,1,256,256)).view(a,self.c,b,256,256)
            x = torch.sum(x,axis = 2)
       

        elif self.preprocessing <2./3:
            x = self.otherpre(input)


        else:
            """
            input = 256 - input
            b1 =  b1 = torch.var(input.view(a,10,256,256),axis=1).view(a,1,256,256)
 #self.preLaurent0(input.view(a,10,256*256)).view(a,1,256,256)
            b2 =  self.preLaurent1(self.norm(input).view(a,10,256*256)).view(a,1,256,256)
            b3 =  self.preLaurent2(input.view(a,10,256*256)).view(a,1,256,256)
            x = torch.cat((b1,b2,b3),axis=1)
            """
            x = torch.cat(((input[:,0:1,:,:]+input[:,1:2,:,:]+input[:,2:3,:,:])/3,
(input[:,3:4,:,:]+input[:,4:5,:,:]+input[:,5:6,:,:])/3,
 (input[:,6:7,:,:]+input[:,7:8,:,:]+input[:,8:9,:,:]+input[:,9:,:,:])/4),axis=1)
        if random.random()>1.99:
            temp = x
            for i in range(len(x)):
                                    fig, axs = plt.subplots(3,4)
                                    axs[0,0].imshow(temp[i,0].detach())
                                    axs[0,1].imshow(temp[i,1].detach())
                                    axs[0,2].imshow(temp[i,2].detach())
                                    axs[1,0].imshow(input[i,0])
                                    axs[1,1].imshow(input[i,1])
                                    axs[1,2].imshow(input[i,2])
                                    axs[2,0].imshow(input[i,3])
                                    axs[2,1].imshow(input[i,4])
                                    axs[2,2].imshow(input[i,5])
                                    axs[0,3].imshow(input[i,6])
                                    axs[1,3].imshow(input[i,7])
                                    axs[2,3].imshow(input[i,8])               
                                   # axs[3,0].imshow(input[i,9])
                                   # axs[3,1].imshow(F.sigmoid(outputs[i][1]-outputs[i][0]))
                                   # axs[1,3].imshow(outputs[i])
                                    #print(outputs[i])
                                    #print(y[i])
                                   # axs[2,3].imshow(temp[i])
                                   # plt.savefig(str(loss)+str(i)+".jpg")
                                    plt.show()
        
       # x = self.pre3(self.pre2(x))



        x= self.model.forward(x)
        return self.post(self.post1(x))
"""
#AdaBelief = torch.optim.SGD
try:
	print("one")
	model = torch.load("modelUNET6A17")
	opt = AdaBelief(model.parameters(), lr = lr,print_change_log = False)
	opt.load_state_dict(torch.load("optUNET6A17"))
except :
	print("two")
	import segmentation_models_pytorch as smp
"""
"""	
	model = smp.Unet(
    encoder_name="resnet34",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7
    encoder_weights="imagenet",     # use `imagenet` pre-trained weights for encoder initialization
    in_channels=13,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)
    classes=1,                      # model output channels (number of classes in your dataset)
	)
""" #deeplabv3_resnet50
"""
	import torchvision

	#model = supa(torchvision.models.shufflenet_v2_x1_0(pretrained=False, progress=True, num_classes=4))
	model = supa(torchvision.models.resnet18(pretrained=True, progress=True, num_classes=1000))
	#model = supa(torchvision.models.densenet121(pretrained=False, progress=True, num_classes=4))
	#model = ResNetUNet(2)
	opt = AdaBelief(model.parameters(), lr = lr,print_change_log = False)

model.train()
train_loss, valid_loss, model, opt = train(model,train_dl,valid_dl,loss_fn,opt,acc_metric, epochs = 5)#,go = 'valid')
#train_loss, valid_loss, model, opt = train(model,train_dl,valid_dl,loss_fn,opt,acc_metric, epochs = 5)#,go = 'valid')
#train_loss, valid_loss, model, opt = train(model,train_dl,valid_dl,loss_fn,opt,acc_metric, epochs = 5)#,go = 'valid')
#train_loss, valid_loss, model, opt = train(model,train_dl,valid_dl,loss_fn,opt,acc_metric, epochs = 5)#,go = 'valid')
#train_loss, valid_loss, model, opt = train(model,train_dl,valid_dl,loss_fn,opt,acc_metric, epochs = 5)#,go = 'valid')
#train_loss, valid_loss, model, opt = train(model,train_dl,valid_dl,loss_fn,opt,acc_metric, epochs = 5)#,go = 'valid')
model.eval()
data = CloudDataset(base_path/'6Xtest', base_path/'6Ytest')
train_dl = DataLoader(data, batch_size=16, shuffle=True)
train_loss, valid_loss, model, opt = train(model,train_dl,valid_dl,loss_fn,opt,acc_metric, go = 'valid' , epochs = 1)
#torch.save(model, "modelUNET6A17")
#torch.save(opt.state_dict(), "optUNET6A17")
print("three")
"""
import os
os.chdir("Documents/")
def doing(string):
    print()
    print(string)
    print(os.system("ls"))
    os.system("cp 6X/"+string+"* 6Xtest/")
    os.system("cp 6Y/"+string+"* 6Ytest/")
    os.system("rm 6X/"+string+"*")
    os.system("rm 6Y/"+string+"*")
    base_path = Path("")
    #data = CloudDataset(base_path/'secondattemptX', base_path/'secondattemptY')
    data = CloudDataset(base_path/'6X', base_path/'6Y')
    import torch 
    import numpy as np
    import cv2
    len(data)
    ratio = .75
    train_ds, valid_ds = torch.utils.data.random_split(data, (int(len(data)*ratio), len(data) - (int(len(data)*ratio))))


    train_dl = DataLoader(data, batch_size=16, shuffle=True)
    valid_dl = [] #DataLoader(valid_ds, batch_size=4, shuffle=True)
    try:
        print("one")
        model = torch.load("modelUNET6A20")
        opt = AdaBelief(model.parameters(), lr = lr,print_change_log = False, weight_decay =  0.12457348241590291)
        opt.load_state_dict(torch.load("optUNET6A20"))
    except :
        print("two")
        import segmentation_models_pytorch as smp
        """	
        model = smp.Unet(
        encoder_name="resnet34",        # choose encoder, e.g. mobilenet_v2 or efficientnet-b7
        encoder_weights="imagenet",     # use `imagenet` pre-trained weights for encoder initialization
        in_channels=13,                  # model input channels (1 for gray-scale images, 3 for RGB, etc.)
        classes=1,                      # model output channels (number of classes in your dataset)
        )
        """#deeplabv3_resnet50
        import torchvision

        #model = supa(torchvision.models.shufflenet_v2_x1_0(pretrained=False, progress=True, num_classes=4))
        model = supa(torchvision.models.resnet18(pretrained=True, progress=True, num_classes=1000))
        #model = supa(torchvision.models.densenet121(pretrained=False, progress=True, num_classes=4))
        #model = ResNetUNet(2)
        wd = 10**(-5*(random.random()))
        wd = 0.0007687495800262013
        wd = 0.12457348241590291
        print(wd)
        opt = AdaBelief(model.parameters(), lr = lr,print_change_log = False, weight_decay =  0.12457348241590291)

    model.train()
    train_loss, valid_loss, model, opt = train(model,train_dl,valid_dl,loss_fn,opt,acc_metric, epochs = 4)#,go = 'valid')
    train_loss, valid_loss, model, opt = train(model,train_dl,valid_dl,loss_fn,opt,acc_metric, epochs = 4)#,go = 'valid')
    train_loss, valid_loss, model, opt = train(model,train_dl,valid_dl,loss_fn,opt,acc_metric, epochs = 4)#,go = 'valid')
    train_loss, valid_loss, model, opt = train(model,train_dl,valid_dl,loss_fn,opt,acc_metric, epochs = 4)#,go = 'valid')
    train_loss, valid_loss, model, opt = train(model,train_dl,valid_dl,loss_fn,opt,acc_metric, epochs = 4)#,go = 'valid')
    train_loss, valid_loss, model, opt = train(model,train_dl,valid_dl,loss_fn,opt,acc_metric, epochs = 4)#,go = 'valid')

    model.eval()
    data = CloudDataset(base_path/'6Xtest', base_path/'6Ytest')
    train_dl = DataLoader(data, batch_size=16, shuffle=True)
    train_loss, valid_loss, model, opt = train(model,train_dl,valid_dl,loss_fn,opt,acc_metric, go = 'valid' , epochs = 1)
    os.system("cp 6Xtest/"+string+"* 6X/")
    os.system("cp 6Ytest/"+string+"* 6Y/")
    os.system("rm 6Xtest/*")
    os.system("rm 6Ytest/*")
    os.system("cd ..")
    torch.save(model, "modelUNET6A20")
    torch.save(opt.state_dict(), "optUNET6A20")
print("starts")
#doing("first")
#doing("second")
#doing("third")
#doing("forth") #good #0.0007687495800262013
#doing("fifth") #usuall
#doing("sixth")
#doing("seventh")
#doing("eighth")
#doing("nineth")
#doing("tenth")
doing("eleventh") #good, 0.12457348241590291
#doing("twelveth")
#doing("thirteenth")
